{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "historical-possibility",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-argument",
   "metadata": {},
   "source": [
    "Vamos pegar uma intuição do k-means inicialmente. Para tanto, vamos gerar algumas bolhas (como se fossem agrupamentos pra entender como o k-means poderia ser usado para agrupar esses dados). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-defeat",
   "metadata": {},
   "source": [
    "Agora, vamos criar nossos dados. Especificamos a localização dos centróides e definimos o desvio padrão dos dados para cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-landing",
   "metadata": {},
   "source": [
    "Agora usamos o método make_blobs para criar as bolhas baseadas nas informações que passamos parâmetros. O método retorna os dados gerados bem como o rótulo de cada um (seu cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-rally",
   "metadata": {},
   "source": [
    "Podemos criar uma função para visualizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-saver",
   "metadata": {},
   "source": [
    "E agora, fazemos o plot das bolhas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-journey",
   "metadata": {},
   "source": [
    "Agora, a ideia é treinar o k-means nesses dados e verificar se ele encontra esses grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-newark",
   "metadata": {},
   "source": [
    "Para treinar o k-means, definimos o número de k. A partir desses dados, 5 é um número adequado. Entretanto, em problemas reais, a definição de k não é tão simples assim, como já vimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-carrier",
   "metadata": {},
   "source": [
    "Podemos comparar os centróides obtidos pelo k-means com os centróides que definimos previamente ao criar os dados. Isso nos dará uma ideia da qualidade dos clusters formados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-withdrawal",
   "metadata": {},
   "source": [
    "Algo interessante de se fazer é plotar as fronteiras de decisão que o algoritmo encontrou e comparar com o plot original dos dados, o que nos permite identificar erros de atribuição a um determinado cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando as funções para visualizar as fronteiras de decisão\n",
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-possession",
   "metadata": {},
   "source": [
    "Agora podemos fazer o plot. O que vamos obter é conhecido como Diagrama de Voronoi, uma forma de representar clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-trauma",
   "metadata": {},
   "source": [
    "Compare essa figura com a figura original das bolhas. Consegue identificar erros de atribuição?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-creation",
   "metadata": {},
   "source": [
    "Fazer a predição de novas amostras é particularmente simples também:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-internet",
   "metadata": {},
   "source": [
    "# O Algoritmo K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-savage",
   "metadata": {},
   "source": [
    "Depois de entendermos intuitivamente o K-means, é hora de nos aprofundarmos na maneira em que ele constrói clusters.\n",
    "\n",
    "Suponha que seja fornecido os centróides. Dessa maneira, poderíamos facilmente rotular todas as intâncias no dataset atribuindo a cada uma delas ao cluster cujo centróide seja o mais próximo. \n",
    "\n",
    "Alternativamente, se todos os rótulos das amostras foram fornecidos, poderíamos localizar os centróides calculando a média de todas as amostras para cada cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-volleyball",
   "metadata": {},
   "source": [
    "Mas e se nada disso for fornecido? \n",
    "\n",
    "Podemos posicionar os centróides de maneira aleatória (selecionando k amostras aleatóriamente e usando suas localizações como centróides). Então, rotulamos as instâncias, atualizamos os centróides, rotulamos as instâncias, atualizamos os centróides e assim sucessivamente até que os centróides parem de se mover. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-powder",
   "metadata": {},
   "source": [
    "Vamos executar o K-means para 1, 2 e 3 iterações e ver como os centróides vão se movendo bem como as instâncias atualizando seus rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=1, random_state=0)\n",
    "kmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=2, random_state=0)\n",
    "kmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
    "                     algorithm=\"full\", max_iter=3, random_state=0)\n",
    "kmeans_iter1.fit(X)\n",
    "kmeans_iter2.fit(X)\n",
    "kmeans_iter3.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_data(X)\n",
    "plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n",
    "plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "plt.tick_params(labelbottom=False)\n",
    "plt.title(\"Update the centroids (initially randomly)\", fontsize=14)\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)\n",
    "plt.title(\"Label the instances\", fontsize=14)\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)\n",
    "plot_centroids(kmeans_iter2.cluster_centers_)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n",
    "plot_centroids(kmeans_iter3.cluster_centers_)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-wholesale",
   "metadata": {},
   "source": [
    "Vimos que, em poucas iterações, o k-means se aproximou da solução ótima. Entretanto, como vimos, o k-means é sensível a inicialização dos centróides, o que signifca que ele pode não convergir para a solução ótima. Vamos entender como podemos encontrar a solução ótima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-chart",
   "metadata": {},
   "source": [
    "### Métodos de Inicialização dos Centróides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-judge",
   "metadata": {},
   "source": [
    "Se sabmos de antemão onde aproximadamente os centróides devem ser posicionados, podemos setar o parâmetro *init* a partir de um Numpy array e setar *n_init* igual a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3,3],[-3,2],[-3,1],[-1,2],[0,2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-chance",
   "metadata": {},
   "source": [
    "Outra solução é executar o algoritmo múltiplas vezes com diferentes inicializações aleatórias e guardar a melhor solução, mas como sabemos que uma solução é a melhor?\n",
    "\n",
    "Temos uma métrica de performance que nos fornece essa informação: inercia do modelo, obtida através do parâmetro *inertia_*. Essa métrica nada mais é que distância média quadrática entre cada instância e o centróide mais próximo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-charm",
   "metadata": {},
   "source": [
    "### Encontrando o número ótimo de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-killer",
   "metadata": {},
   "source": [
    "Até o momento, definimos o número de cluster (k) igual a 5 porque é o número óbvio quando se olha os dados. Entretanto, de modo geral, não será fácil saber definir k e os resultados podem ser bem ruins se escolhermos o número incorreto de k. \n",
    "\n",
    "Observe o exemplo quando usamos k=3 e depois k=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n",
    "    clusterer1.fit(X)\n",
    "    clusterer2.fit(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_decision_boundaries(clusterer1, X)\n",
    "    if title1:\n",
    "        plt.title(title1, fontsize=14)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
    "    if title2:\n",
    "        plt.title(title2, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-liberal",
   "metadata": {},
   "source": [
    "E se usarmos a inércia para definir o número ótimo de clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-repeat",
   "metadata": {},
   "source": [
    "Qual o problema dessa solução?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-poultry",
   "metadata": {},
   "source": [
    "Não podemos simplesmente usar o valor de k que minimiza a inércia visto que ela continuará diminuindo à medida que aumentamos o valor de k. \n",
    "\n",
    "De fato, quanto mais clusters existem, mais perto cada instância ficará do seu centróide e, portanto, menor o valor da iníercia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-independence",
   "metadata": {},
   "source": [
    "#### Elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-clerk",
   "metadata": {},
   "source": [
    "Entretanto, ainda podemos usar a inércia para definir um valor adequado de clusters. Ao analisarmos o gráfico, observamos que o valor na inércia cai drásticamente à medida que aumentamos o valor de k até 4 e, então, o valor da inércia diminui muito mais lentamente a partir daí. Esta curva possui vagamente uma forma de um braço e possui um \"cotovelo\" (elbow) quando k=4. Assim, poderíamos usar k=4 para uma solução adequada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow',\n",
    "             xy=(4, inertias[3]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(kmeans_per_k[4-1], X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-chest",
   "metadata": {},
   "source": [
    "#### Silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-dialogue",
   "metadata": {},
   "source": [
    "Uma outra, e melhor, abordagem para determinar o número de cluster é a partir do *silhouette coefficient*. Como vimos, esse coeficiente é dado pela seguinte equação:\n",
    "\n",
    "$\\frac{(b-a)}{max(a,b)}$\n",
    "\n",
    "em que:\n",
    "\n",
    "* é a distância média para outras amostras no mesmo cluster (distância média intra-cluster)\n",
    "* 𝑏 é a distância média do cluster mais próximo (distância média para as amostras do próximo cluster mais próximo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-defensive",
   "metadata": {},
   "source": [
    "Para calcular o *silhouette score* podemos usar a função de mesmo nome da Scikit-Learn, fornecendo todas as amostras do dataset e seus respectivos rótulos aos quais foram atribuídos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-landing",
   "metadata": {},
   "source": [
    "Podemos comparar o valor desse score para diferentes números de clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-granny",
   "metadata": {},
   "source": [
    "Como podemos ver, esta visualização é muito mais rica que a anterior (usando *inertia*). Embora confirme que k=4 é uma ótima escolha (da mesma maneira que o gráfico anterior), mostra também que k=5 é uma excelente opção e muito melhor que k=6 ou k=7, o que não é visível quando usamos inércia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-mounting",
   "metadata": {},
   "source": [
    "# Hierarchical Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-education",
   "metadata": {},
   "source": [
    "Para começar, vamos criar alguns dados para entender a formação dos dendrogramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[5,3],\n",
    "    [10,15],\n",
    "    [15,12],\n",
    "    [24,10],\n",
    "    [30,30],\n",
    "    [85,70],\n",
    "    [71,80],\n",
    "    [60,78],\n",
    "    [70,55],\n",
    "    [80,91],])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-maine",
   "metadata": {},
   "source": [
    "Agora, vamos plotar esses dados para visualizar seu comportamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = range(1, 11)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "plt.scatter(X[:,0],X[:,1], label='True Position')\n",
    "\n",
    "for label, x, y in zip(labels, X[:, 0], X[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-3, 3),\n",
    "        textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-jacksonville",
   "metadata": {},
   "source": [
    "É possível ver que esses dados formam dois cluster. A partir deles, vamos, então, desenhar os dendrogramas. A princípio, iremos usar a biblioteca scipy (de onde vem o numpy) para isso. Depois, vamos para Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "linked = linkage(X, 'single')\n",
    "\n",
    "labelList = range(1, 11)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            labels=labelList,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-packet",
   "metadata": {},
   "source": [
    "O algoritmo começa encontrando os dois pontos mais próximos um ao outro baseado na distância euclidiana. No caso, os pontos 2 e 3 são os mais próximos, por isso eles formam o primeiro cluster. \n",
    "\n",
    "Nos dendrogramas, a altura das barras de cada agrupamento determina o valor da distância euclidiana. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-edinburgh",
   "metadata": {},
   "source": [
    "## Hclust usando Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-mention",
   "metadata": {},
   "source": [
    "Vamos agora usar a scikit learn para criar um cluster hierárquico. \n",
    "\n",
    "Primeiro, vamos criar os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[5,3],\n",
    "    [10,15],\n",
    "    [15,12],\n",
    "    [24,10],\n",
    "    [30,30],\n",
    "    [85,70],\n",
    "    [71,80],\n",
    "    [60,78],\n",
    "    [70,55],\n",
    "    [80,91],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "cluster.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-piece",
   "metadata": {},
   "source": [
    "Observe que setamos o número de cluster *n_clusters* igual a 2, usamos a distância euclidiana para calcular a distância entre os pontos e linkage estamos usando a opção \"ward\", que se refere a minimização da variância dos clusters sendo agrupados. As outras opções são as que vimos em sala: single linkage, complete linkage e average linkage\n",
    "\n",
    "O método fit_predict() retorna o rótulo de cada instância"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-moscow",
   "metadata": {},
   "source": [
    "Podemos plotar os dados originais pintando cada instância de acordo com o cluster a que ela foi atribuída:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=cluster.labels_, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-system",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
